{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1OzQFcm-q-Jv_TFot9znPs_BxxcEBUJpe",
      "authorship_tag": "ABX9TyPP9wHTPjoLXOOSNPx9jdwU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKrX1Bi7DuzR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7773b555-b214-4606-95b5-c44f4f5d1576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore) (1.26.4)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from fvcore) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (2.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from fvcore) (11.1.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from iopath>=0.1.7->fvcore) (4.12.2)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61396 sha256=d19a1b3103a92bcce59e3d7d41d2eb864b3dbc2ea06cdd97297664300828109e\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/71/95/3b8fde5c65c6e4a806e0867c1651dcc71a1cb2f3430e8f355f\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=aa4baaa1800e40bd49ccf9a0edf870b6031f4b9da281eaf30c34187c4a8f891a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/5e/16/6117f8fe7e9c0c161a795e10d94645ebcf301ccbd01f66d8ec\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.1.1 yacs-0.1.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/pytorchvideo/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/X3D_S.pyth\" to /root/.cache/torch/hub/checkpoints/X3D_S.pyth\n",
            "100%|██████████| 29.4M/29.4M [00:00<00:00, 196MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting av\n",
            "  Downloading av-14.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Downloading av-14.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.5/39.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-14.1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "!pip install fvcore\n",
        "# Choose the `x3d_s` model\n",
        "model_name = 'x3d_s'\n",
        "model = torch.hub.load('facebookresearch/pytorchvideo', model_name, pretrained=True)\n",
        "\n",
        "\n",
        "import json\n",
        "import urllib\n",
        "!pip install av\n",
        "from pytorchvideo.data.encoded_video import EncodedVideo\n",
        "\n",
        "from torchvision.transforms import Compose, Lambda\n",
        "from torchvision.transforms._transforms_video import (\n",
        "    CenterCropVideo,\n",
        "    NormalizeVideo,\n",
        ")\n",
        "from pytorchvideo.transforms import (\n",
        "    ApplyTransformToKey,\n",
        "    ShortSideScale,\n",
        "    UniformTemporalSubsample\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set to GPU or CPU\n",
        "device = \"cpu\"\n",
        "model = model.eval()\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "n9BAh8UJXRjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_url = \"https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\"\n",
        "json_filename = \"kinetics_classnames.json\"\n",
        "try: urllib.URLopener().retrieve(json_url, json_filename)\n",
        "except: urllib.request.urlretrieve(json_url, json_filename)\n",
        "\n",
        "with open(json_filename, \"r\") as f:\n",
        "    kinetics_classnames = json.load(f)\n",
        "\n",
        "# Create an id to label name mapping\n",
        "kinetics_id_to_classname = {}\n",
        "for k, v in kinetics_classnames.items():\n",
        "    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")"
      ],
      "metadata": {
        "id": "TRmnYhViXUHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean = [0.45, 0.45, 0.45]\n",
        "std = [0.225, 0.225, 0.225]\n",
        "frames_per_second = 30\n",
        "model_transform_params  = {\n",
        "    \"x3d_xs\": {\n",
        "        \"side_size\": 182,\n",
        "        \"crop_size\": 182,\n",
        "        \"num_frames\": 4,\n",
        "        \"sampling_rate\": 12,\n",
        "    },\n",
        "    \"x3d_s\": {\n",
        "        \"side_size\": 182,\n",
        "        \"crop_size\": 182,\n",
        "        \"num_frames\": 13,\n",
        "        \"sampling_rate\": 6,\n",
        "    },\n",
        "    \"x3d_m\": {\n",
        "        \"side_size\": 256,\n",
        "        \"crop_size\": 256,\n",
        "        \"num_frames\": 16,\n",
        "        \"sampling_rate\": 5,\n",
        "    }\n",
        "}\n",
        "\n",
        "# Get transform parameters based on model\n",
        "transform_params = model_transform_params[model_name]\n",
        "\n",
        "# Note that this transform is specific to the slow_R50 model.\n",
        "transform =  ApplyTransformToKey(\n",
        "    key=\"video\",\n",
        "    transform=Compose(\n",
        "        [\n",
        "            UniformTemporalSubsample(transform_params[\"num_frames\"]),\n",
        "            Lambda(lambda x: x/255.0),\n",
        "            NormalizeVideo(mean, std),\n",
        "            ShortSideScale(size=transform_params[\"side_size\"]),\n",
        "            CenterCropVideo(\n",
        "                crop_size=(transform_params[\"crop_size\"], transform_params[\"crop_size\"])\n",
        "            )\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "# The duration of the input clip is also specific to the model.\n",
        "clip_duration = (transform_params[\"num_frames\"] * transform_params[\"sampling_rate\"])/frames_per_second"
      ],
      "metadata": {
        "id": "XuDBYFUTXxIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#url_link = \"https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4\"\n",
        "\n",
        "# try: urllib.URLopener().retrieve(url_link, video_path)\n",
        "# except: urllib.request.urlretrieve(url_link, video_path)\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "video = '/content/drive/MyDrive/Card_dealing.mp4'\n",
        "vid_capture=cv.VideoCapture(video)\n",
        "\n",
        "while vid_capture.isOpened():\n",
        "    ret, frame = vid_capture.read()\n",
        "    resized_frame = cv.resize(frame, (300, 300))\n",
        "\n",
        "    # if frame is read correctly ret is True\n",
        "    if not ret:\n",
        "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
        "        break\n",
        "    gray = cv.cvtColor(resized_frame, cv.COLOR_BGR2GRAY)\n",
        "\n",
        "    cv2_imshow(gray)\n",
        "    cv.destroyAllWindows()\n",
        "    if cv.waitKey(1) == ord('q'):\n",
        "        break\n",
        "\n",
        "vid_capture.release()\n",
        "cv.destroyAllWindows()"
      ],
      "metadata": {
        "id": "g-cgKm5yYEa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the duration of the clip to load by specifying the start and end duration\n",
        "# The start_sec should correspond to where the action occurs in the video\n",
        "start_sec = 0\n",
        "end_sec = start_sec + clip_duration\n",
        "\n",
        "# Initialize an EncodedVideo helper class and load the video\n",
        "video = EncodedVideo.from_path(video)\n",
        "# Load the desired clip\n",
        "video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
        "\n",
        "# Apply a transform to normalize the video input\n",
        "video_data = transform(video_data)\n",
        "\n",
        "# Move the inputs to the desired device\n",
        "inputs = video_data[\"video\"]\n",
        "inputs = inputs.to(device)"
      ],
      "metadata": {
        "id": "hL5gPC09YLLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the input clip through the model\n",
        "preds = model(inputs[None, ...])\n",
        "\n",
        "# Get the predicted classes\n",
        "post_act = torch.nn.Softmax(dim=1)\n",
        "preds = post_act(preds)\n",
        "pred_classes = preds.topk(k=5).indices[0]\n",
        "\n",
        "# Map the predicted classes to the label names\n",
        "pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]\n",
        "print(\"Top 5 predicted labels: %s\" % \", \".join(pred_class_names))"
      ],
      "metadata": {
        "id": "w47Iw5H8YSsf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbaa86c7-5a0f-477e-cecf-44efcda4bd16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 predicted labels: playing cards, shuffling cards, playing poker, bookbinding, wrapping present\n"
          ]
        }
      ]
    }
  ]
}